use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::Instant;

use actix::{Addr, System};
use futures::{future, FutureExt};
use log::info;

use integration_tests::test_helpers::heavy_test;
use near_actix_test_utils::run_actix;
use near_chunks::{
    CHUNK_REQUEST_RETRY_MS, CHUNK_REQUEST_SWITCH_TO_FULL_FETCH_MS,
    CHUNK_REQUEST_SWITCH_TO_OTHERS_MS,
};
use near_client::test_utils::setup_mock_all_validators;
use near_client::{ClientActor, GetBlock, ViewClientActor};
use near_logger_utils::init_test_logger;
use near_network::types::AccountIdOrPeerTrackingShard;
use near_network::{NetworkClientMessages, NetworkRequests, NetworkResponses, PeerInfo};
use near_primitives::hash::CryptoHash;
use near_primitives::transaction::SignedTransaction;
use near_primitives::types::AccountId;

/// Runs block producing client and stops after network mock received seven blocks
/// Confirms that the blocks form a chain (which implies the chunks are distributed).
/// Confirms that the number of messages transmitting the chunks matches the expected number.
fn chunks_produced_and_distributed_common(
    validator_groups: u64,
    drop_from_1_to_4: bool,
    block_timeout: u64,
) {
    init_test_logger();

    let connectors: Arc<RwLock<Vec<(Addr<ClientActor>, Addr<ViewClientActor>)>>> =
        Arc::new(RwLock::new(vec![]));
    let heights = Arc::new(RwLock::new(HashMap::new()));
    let heights1 = heights.clone();

    let height_to_hash = Arc::new(RwLock::new(HashMap::new()));
    let height_to_epoch = Arc::new(RwLock::new(HashMap::new()));

    let check_height =
        move |hash: CryptoHash, height| match heights1.write().unwrap().entry(hash.clone()) {
            Entry::Occupied(entry) => {
                assert_eq!(*entry.get(), height);
            }
            Entry::Vacant(entry) => {
                entry.insert(height);
            }
        };

    let validators = vec![
        vec![
            "test1".parse().unwrap(),
            "test2".parse().unwrap(),
            "test3".parse().unwrap(),
            "test4".parse().unwrap(),
        ],
        vec![
            "test5".parse().unwrap(),
            "test6".parse().unwrap(),
            "test7".parse().unwrap(),
            "test8".parse().unwrap(),
        ],
    ];
    let key_pairs = (0..8).map(|_| PeerInfo::random()).collect::<Vec<_>>();

    let mut partial_chunk_msgs = 0;
    let mut partial_chunk_request_msgs = 0;

    let (_, conn, _) = setup_mock_all_validators(
        validators.clone(),
        key_pairs.clone(),
        validator_groups,
        true,
        block_timeout,
        false,
        false,
        5,
        true,
        vec![false; validators.iter().map(|x| x.len()).sum()],
        vec![true; validators.iter().map(|x| x.len()).sum()],
        false,
        Arc::new(RwLock::new(Box::new(move |from_whom: AccountId, msg: &NetworkRequests| {
            match msg {
                NetworkRequests::Block { block } => {
                    check_height(*block.hash(), block.header().height());
                    check_height(*block.header().prev_hash(), block.header().height() - 1);

                    let h = block.header().height();

                    let mut height_to_hash = height_to_hash.write().unwrap();
                    height_to_hash.insert(h, *block.hash());

                    let mut height_to_epoch = height_to_epoch.write().unwrap();
                    height_to_epoch.insert(h, block.header().epoch_id().clone());

                    println!(
                            "[{:?}]: BLOCK {} HEIGHT {}; HEADER HEIGHTS: {} / {} / {} / {};\nAPPROVALS: {:?}",
                            Instant::now(),
                            block.hash(),
                            block.header().height(),
                            block.chunks()[0].height_created(),
                            block.chunks()[1].height_created(),
                            block.chunks()[2].height_created(),
                            block.chunks()[3].height_created(),
                            block.header().approvals(),
                        );

                    if h > 1 {
                        // Make sure doomslug finality is computed correctly.
                        assert_eq!(
                            block.header().last_ds_final_block(),
                            height_to_hash.get(&(h - 1)).unwrap()
                        );

                        // Make sure epoch length actually corresponds to the desired epoch length
                        // The switches are expected at 0->1, 5->6 and 10->11
                        let prev_epoch_id = height_to_epoch.get(&(h - 1)).unwrap().clone();
                        assert_eq!(block.header().epoch_id() == &prev_epoch_id, h % 5 != 1);

                        // Make sure that the blocks leading to the epoch switch have twice as
                        // many approval slots
                        assert_eq!(block.header().approvals().len() == 8, h % 5 == 0 || h % 5 == 4);
                    }
                    if h > 2 {
                        // Make sure BFT finality is computed correctly
                        assert_eq!(
                            block.header().last_final_block(),
                            height_to_hash.get(&(h - 2)).unwrap()
                        );
                    }

                    if block.header().height() > 1 {
                        for shard_id in 0..4 {
                            // If messages from 1 to 4 are dropped, 4 at their heights will
                            //    receive the block significantly later than the chunks, and
                            //    thus would discard the chunks
                            if !drop_from_1_to_4 || block.header().height() % 4 != 3 {
                                assert_eq!(
                                    block.header().height(),
                                    block.chunks()[shard_id].height_created()
                                );
                            }
                        }
                    }

                    if block.header().height() >= 12 {
                        println!("PREV BLOCK HASH: {}", block.header().prev_hash());
                        println!(
                            "STATS: responses: {} requests: {}",
                            partial_chunk_msgs, partial_chunk_request_msgs
                        );

                        System::current().stop();
                    }
                }
                NetworkRequests::PartialEncodedChunkMessage {
                    account_id: to_whom,
                    partial_encoded_chunk: _,
                } => {
                    partial_chunk_msgs += 1;
                    if drop_from_1_to_4
                        && from_whom.as_ref() == "test1"
                        && to_whom.as_ref() == "test4"
                    {
                        println!("Dropping Partial Encoded Chunk Message from test1 to test4");
                        return (NetworkResponses::NoResponse, false);
                    }
                }
                NetworkRequests::PartialEncodedChunkForward { account_id: to_whom, .. } => {
                    if drop_from_1_to_4
                        && from_whom.as_ref() == "test1"
                        && to_whom.as_ref() == "test4"
                    {
                        println!(
                            "Dropping Partial Encoded Chunk Forward Message from test1 to test4"
                        );
                        return (NetworkResponses::NoResponse, false);
                    }
                }
                NetworkRequests::PartialEncodedChunkResponse { route_back: _, response: _ } => {
                    partial_chunk_msgs += 1;
                }
                NetworkRequests::PartialEncodedChunkRequest {
                    target: AccountIdOrPeerTrackingShard { account_id: Some(to_whom), .. },
                    request: _,
                } => {
                    if drop_from_1_to_4
                        && from_whom.as_ref() == "test4"
                        && to_whom.as_ref() == "test1"
                    {
                        info!("Dropping Partial Encoded Chunk Request from test4 to test1");
                        return (NetworkResponses::NoResponse, false);
                    }
                    if drop_from_1_to_4
                        && from_whom.as_ref() == "test4"
                        && to_whom.as_ref() == "test2"
                    {
                        info!("Observed Partial Encoded Chunk Request from test4 to test2");
                    }
                    partial_chunk_request_msgs += 1;
                }
                _ => {}
            };
            (NetworkResponses::NoResponse, true)
        }))),
    );
    *connectors.write().unwrap() = conn;

    let view_client = connectors.write().unwrap()[0].1.clone();
    actix::spawn(view_client.send(GetBlock::latest()).then(move |res| {
        let block_hash = res.unwrap().unwrap().header.hash;
        let connectors_ = connectors.write().unwrap();
        connectors_[0].0.do_send(NetworkClientMessages::Transaction {
            transaction: SignedTransaction::empty(block_hash),
            is_forwarded: false,
            check_only: false,
        });
        connectors_[1].0.do_send(NetworkClientMessages::Transaction {
            transaction: SignedTransaction::empty(block_hash),
            is_forwarded: false,
            check_only: false,
        });
        connectors_[2].0.do_send(NetworkClientMessages::Transaction {
            transaction: SignedTransaction::empty(block_hash),
            is_forwarded: false,
            check_only: false,
        });
        future::ready(())
    }));
}

#[test]
fn chunks_produced_and_distributed_all_in_all_shards() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(1, false, 15 * CHUNK_REQUEST_RETRY_MS);
        });
    });
}

#[test]
fn chunks_produced_and_distributed_2_vals_per_shard() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(2, false, 15 * CHUNK_REQUEST_RETRY_MS);
        });
    });
}

#[test]
fn chunks_produced_and_distributed_one_val_per_shard() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(4, false, 15 * CHUNK_REQUEST_RETRY_MS);
        });
    });
}

/// The timeout for requesting chunk from others is 1s. 3000 block timeout means that a participant
/// that is otherwise ready to produce a block will wait for 3000/2 milliseconds for all the chunks.
/// We block all the communication from test1 to test4, and expect that in 1.5 seconds test4 will
/// give up on getting the part from test1 and will get it from test2 (who will have it because
/// `validator_groups=2`)
#[test]
fn chunks_recovered_from_others() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(2, true, 4 * CHUNK_REQUEST_SWITCH_TO_OTHERS_MS);
        });
    });
}

/// Same test as above, but the number of validator groups is four, therefore test2 doesn't have the
/// part test4 needs. The only way test4 can recover the part is by reconstructing the whole chunk,
/// but they won't do it for the first 3 seconds, and 3s block_timeout means that the block producers
/// only wait for 3000/2 milliseconds until they produce a block with some chunks missing
#[test]
#[should_panic]
fn chunks_recovered_from_full_timeout_too_short() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(4, true, 2 * CHUNK_REQUEST_SWITCH_TO_OTHERS_MS);
        });
    });
}

/// Same test as above, but the timeout is sufficiently large for test4 now to reconstruct the full
/// chunk
#[test]
fn chunks_recovered_from_full() {
    heavy_test(|| {
        run_actix(async {
            chunks_produced_and_distributed_common(
                4,
                true,
                2 * CHUNK_REQUEST_SWITCH_TO_FULL_FETCH_MS,
            );
        })
    });
}
